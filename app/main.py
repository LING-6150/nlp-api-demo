import os
import sys
import logging
import json
from typing import List
from fastapi import FastAPI, Query, Body

# ä»æ–°æ¨¡å—å¯¼å…¥å„éƒ¨åˆ†é€»è¾‘
from app.services.predictor import predict_topic as predict_topic_logic
from app.services.logger import write_log, init_csv
from app.utils.redis_client import get_from_cache, save_to_cache
from app.config import REDIS_HOST, REDIS_PORT, CACHE_EXPIRE_SECONDS

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
if not os.path.exists("logs"):
    os.makedirs("logs")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("logs/app.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("nlp-api")

app = FastAPI()

# åˆå§‹åŒ– CSV æ–‡ä»¶ï¼ˆå†™å…¥è¡¨å¤´ï¼‰
init_csv()

@app.get("/predict/")
def predict(text: str = Query(..., description="è®ºæ–‡æ‘˜è¦å†…å®¹")):
    logger.info(f"ğŸ“¥ æ”¶åˆ°è¯·æ±‚æ–‡æœ¬: {text}")
    cache_key = f"prediction:{text}"
    
    # å°è¯•ä» Redis ç¼“å­˜è·å–ç»“æœ
    result = get_from_cache(cache_key)
    if result:
        logger.info("âœ… å‘½ä¸­ç¼“å­˜ï¼ˆRedisï¼‰")
        write_log(text, result["topic"], result["confidence"], "redis_cache")
        result["source"] = "redis_cache"
        return result

    # è°ƒç”¨ predictor æ¨¡å—å¤„ç†ä¸šåŠ¡é€»è¾‘
    result = predict_topic_logic(text)
    logger.info(f"ğŸ§  æ¨¡å‹æ¨ç†ç»“æœ: {result}")
    
    # å°†ç»“æœä¿å­˜åˆ° Redisï¼Œå¹¶å†™å…¥æ—¥å¿—
    save_to_cache(cache_key, result, CACHE_EXPIRE_SECONDS)
    write_log(text, result["topic"], result["confidence"], "model_inference")
    result["source"] = "model_inference"
    return result

@app.post("/batch_predict/")
def batch_predict(texts: List[str] = Body(..., description="è®ºæ–‡æ‘˜è¦åˆ—è¡¨")):
    results = []
    for text in texts:
        logger.info(f"ğŸ“¥ æ‰¹é‡è¯·æ±‚æ–‡æœ¬: {text}")
        cache_key = f"prediction:{text}"
        result = get_from_cache(cache_key)
        if result:
            logger.info("âœ… å‘½ä¸­ç¼“å­˜ï¼ˆRedisï¼‰")
            source = "redis_cache"
        else:
            result = predict_topic_logic(text)
            logger.info(f"ğŸ§  æ¨¡å‹æ¨ç†ç»“æœ: {result}")
            save_to_cache(cache_key, result, CACHE_EXPIRE_SECONDS)
            source = "model_inference"
        write_log(text, result["topic"], result["confidence"], source)
        result["source"] = source
        result["text"] = text
        results.append(result)
    return results
