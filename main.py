import sys
import os
import logging
from typing import List
from fastapi import FastAPI, Query, Body
import redis
from utils import fake_nlp_prediction

# æ—¥å¿—ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨å°±åˆ›å»ºï¼‰
if not os.path.exists("logs"):
    os.makedirs("logs")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("logs/app.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("nlp-api")

# å¼ºåˆ¶å°†å½“å‰ç›®å½•åŠ å…¥ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

app = FastAPI()

# è¿æ¥ Redis
r = redis.Redis(host='localhost', port=6379, decode_responses=True)

@app.get("/predict/")
def predict_topic(text: str = Query(..., description="è®ºæ–‡æ‘˜è¦å†…å®¹")):
    logger.info(f"ğŸ“¥ æ”¶åˆ°è¯·æ±‚æ–‡æœ¬: {text}")
    cache_key = f"prediction:{text}"
    if r.exists(cache_key):
        result = r.get(cache_key)
        logger.info("âœ… å‘½ä¸­ç¼“å­˜ï¼ˆRedisï¼‰")
        return {"topic": result, "source": "redis_cache"}
    result = fake_nlp_prediction(text)
    logger.info(f"ğŸ§  æ¨¡å‹æ¨ç†ç»“æœ: {result}")
    r.setex(cache_key, 3600, result)
    return {"topic": result, "source": "model_inference"}

@app.post("/batch_predict/")
def batch_predict(texts: List[str] = Body(..., description="è®ºæ–‡æ‘˜è¦åˆ—è¡¨")):
    results = []
    for text in texts:
        logger.info(f"ğŸ“¥ æ”¶åˆ°æ‰¹é‡è¯·æ±‚æ–‡æœ¬: {text}")
        cache_key = f"prediction:{text}"
        if r.exists(cache_key):
            result = r.get(cache_key)
            logger.info("âœ… å‘½ä¸­ç¼“å­˜ï¼ˆRedisï¼‰")
            source = "redis_cache"
        else:
            result = fake_nlp_prediction(text)
            logger.info(f"ğŸ§  æ¨¡å‹æ¨ç†ç»“æœ: {result}")
            r.setex(cache_key, 3600, result)
            source = "model_inference"
        results.append({"text": text, "topic": result, "source": source})
    return results
