import sys
import os
import logging

# æ—¥å¿—ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨å°±åˆ›å»ºï¼‰
if not os.path.exists("logs"):
    os.makedirs("logs")

# é…ç½®æ—¥å¿—ï¼šå†™å…¥æ–‡ä»¶ + åŒæ—¶æ‰“å°ç»ˆç«¯
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("logs/app.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger("nlp-api")

# å¼ºåˆ¶å°†å½“å‰ç›®å½•åŠ å…¥ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fastapi import FastAPI, Query
import redis
from utils import fake_nlp_prediction

app = FastAPI()

# è¿æ¥ Redis
r = redis.Redis(host='localhost', port=6379, decode_responses=True)

@app.get("/predict/")
def predict_topic(text: str = Query(..., description="è®ºæ–‡æ‘˜è¦å†…å®¹")):
    logger.info(f"ğŸ“¥ æ”¶åˆ°è¯·æ±‚æ–‡æœ¬: {text}")

    cache_key = f"prediction:{text}"

    if r.exists(cache_key):
        result = r.get(cache_key)
        logger.info("âœ… å‘½ä¸­ç¼“å­˜ï¼ˆRedisï¼‰")
        return {"topic": result, "source": "redis_cache"}
    
    result = fake_nlp_prediction(text)
    logger.info(f"ğŸ§  æ¨¡å‹æ¨ç†ç»“æœ: {result}")
    r.setex(cache_key, 3600, result)
    return {"topic": result, "source": "model_inference"}
